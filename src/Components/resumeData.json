{
  "resumeData": {
    "theme": "neubrutalism",
    "main": {
      "name": "Deepanshu Lulla",
      "occupation": "AI Infrastructure & Backend Engineer",
      "description": "Building scalable AI infrastructure platforms, distributed backend systems, and production-grade ML/AI services that power enterprise applications",
      "image": "profilepic.jpg",
      "bio": "Deepanshu Lulla is an AI Infrastructure and Backend Engineer with a proven track record of architecting and building large-scale distributed systems and AI/ML platforms. He has led the development of enterprise-level LLM gateways, vector database platforms, and ML model deployment pipelines that process billions of transactions annually. Deepanshu builds fault-tolerant, high-performance backend systems that seamlessly integrate AI capabilities, ensuring optimal scalability, reliability, and observability. His work spans distributed systems architecture, cloud-native infrastructure, MLOps, and building developer-friendly AI platform services that accelerate organizational AI adoption.",
      "email": "deepanshu.lulla@gmail.com",
      "phone": "857-452-0190",
      "address": {
        "street": "",
        "city": "New York City",
        "state": "NY",
        "zip": "10010"
      },
      "website": "http://deepanshululla.com",
      "resumedownload": "https://www.dropbox.com/s/xt816uor9asv12f/Deepanshu%20Lulla%20software%20engineer.pdf?dl=0",
      "social": [
        {
          "name": "linkedin",
          "url": "https://linkedin.com/in/deepanshululla",
          "className": "fa fa-linkedin"
        },
        {
          "name": "github",
          "url": "https://github.com/deepanshululla",
          "className": "fa fa-github"
        }
      ]
    },
    "resume": {
      "education": [
        {
          "school": "Northeastern University,Boston,MA",
          "degree": "Masters(Computer Networking)",
          "graduated": "September 2017",
          "description": "Relevent courses: Computer networks, Network security, Cloud computing, Algorithms"
        },
        {
          "school": "MANIT(NIT Bhopal), India",
          "degree": "Bachelors(Electronics and Communication)",
          "graduated": "May 2014",
          "description": "Had 3 IEEE publication during undergrad level primarily focussed on Embedded systems and robotics"
        }
      ],
      "work": [
        {
          "company": "Bill.com",
          "title": "Staff Software Engineer - AI Infrastructure",
          "years": "June 2024 - Present",
          "description": [
            "Tech Stack: Python, OpenSearch, Kafka, Schema Registry, SQS, Postgres, MySQL, Redis, AWS, Terraform, PyTorch, OpenAI, Gemini, Bedrock, FastAPI, SageMaker, Splunk, Datadog, Google Vision OCR, Document AI, LiteLLM",
            "- Architected and built enterprise-grade LLM Gateway as a Platform Service, enabling organization-wide AI adoption with unified API interfaces, rate limiting, cost tracking, and multi-provider abstraction (OpenAI, Anthropic, Google, AWS Bedrock).",
            "- Designed and implemented Vector Database Platform Service for semantic search and RAG applications, supporting millions of embeddings with sub-millisecond query latency and horizontal scaling capabilities.",
            "- Built production ML model deployment infrastructure on AWS SageMaker, deploying sensitive models (document extraction, credit card approvals) with 99.9% uptime, generating $300M+ annually in transaction processing volume.",
            "- Led cross-functional initiative to unify ML model deployment infrastructure, reducing deployment time by 90% and generating $5.2M annual revenue while migrating from legacy H2O platform.",
            "- Architected Document Intelligence Pipeline with automated categorization, routing, and processing agents, implementing feedback loops and observability for continuous model improvement.",
            "- Established AI governance framework and security guardrails in collaboration with InfoSec and architecture teams, ensuring compliant and safe AI usage across the organization."
          ]
        },
        {
          "company": "Mailchimp (Intuit)",
          "title": "Senior Software Engineer / Tech Lead - Backend Infrastructure",
          "years": "July 2022 - June 2024",
          "description": [
            "Tech Stack: Java, Spring Boot, Spring Data, GraphQL, PHP, Protobufs, Twirp, MySQL, Elasticsearch, GCP, AWS, Anomaly Detection, ML Models",
            "- Architected and led development of Custom Reports platform, a customer-facing analytics product processing petabytes of data. Led team of 5 engineers, delivered in 4 months, generating $5M+ annual revenue.",
            "- Built scalable data aggregation backend consolidating multiple data sources, transforming raw events into business metrics with real-time filtering, grouping, and multi-dimensional analysis capabilities.",
            "- Architected Customer Data Platform backend infrastructure using Change Data Capture (CDC) with Kafka and Debezium, creating unified customer identity resolution system handling millions of records.",
            "- Designed and optimized high-throughput data pipeline processing 10B+ events and 30M+ jobs daily, reducing aggregation job latency from 600s to 2.4s (99.6% improvement) through distributed processing and caching strategies.",
            "- Led AI/ML infrastructure initiatives, building ML model serving APIs and integrating anomaly detection, peer benchmarking, and predictive models into backend services.",
            "- Implemented prompt engineering infrastructure and A/B testing framework for AI-powered analytics features, enabling iterative model improvement and feature experimentation."
          ]
        },
        {
          "company": "Bloomberg LP",
          "title": "Senior Software Engineer - Financial Systems Backend",
          "years": "Oct 2020 - June 2022",
          "description": [
            "Tech Stack: C++, Python, RabbitMQ, Comdb2, Splunk, Kafka, Jenkins, Google Test",
            "- Led critical time-sensitive backend system development with team of 3 engineers, delivering high-availability financial trading systems with zero-downtime deployments.",
            "- Built robust backend services using test-driven development, achieving 95%+ code coverage with comprehensive unit and integration test suites.",
            "- Architected CI/CD infrastructure automating end-to-end testing and deployment pipelines, reducing release cycles from weeks to days.",
            "- Developed Python-based observability and triage tools for financial systems, including late trades analyzer and FIX message parser, reducing incident resolution time by 60%."
          ]
        },
        {
          "company": "Oracle Corp.",
          "title": "Senior Software Developer - Cloud Infrastructure",
          "years": "Dec 2018 - Aug 2020",
          "description": [
            "Tech Stack: Python3, Celery, RabbitMQ, Postgres, Elasticsearch, Kubernetes, Prometheus, Grafana, Helm, Jenkins",
            "- Built resilient distributed ETL pipeline infrastructure with automatic retry mechanisms and circuit breakers, reducing system downtime by 75%.",
            "- Architected RESTful API backend for ETL job management and monitoring, providing real-time job status, metrics, and control interfaces.",
            "- Designed comprehensive observability stack with custom Prometheus exporters, Grafana dashboards, and PagerDuty integration, enabling proactive monitoring and automated incident response.",
            "- Implemented auto-healing mechanisms for distributed pipelines, automatically recovering from failures and reducing manual intervention by 80%.",
            "- Optimized backend performance through load testing and bottleneck analysis, scaling system to handle 200% traffic increase while improving response times by 75%.",
            "- Built end-to-end testing framework for backend APIs, integrated with Kubernetes-based CI/CD pipelines, establishing quality gates for production deployments."
          ]
        },
        {
          "company": "Akamai Technologies Inc.",
          "title": "Software Engineer",
          "years": "Oct 2017 - Nov 2018",
          "description": [
            "- Designed and developed a monitoring platform using Python Flask, Influxdb, React to display the aggregate analysis of time series data.",
            "- Customized Open source monitoring tools like Telegraf, Grafana using Golang to meet the productâ€™s monitoring requirements decreasing troubleshooting time during incidents by more than 50%.",
            "- CCM(Customer Cluster Mapper): Created REST API using Python Flask, Celery, Redis as well as a dashboard using REACT to map different customers to their respective clusters on AWS Opsworks."
          ]
        },
        {
          "company": "Dell EMC",
          "title": "Software Engineer Intern",
          "years": "May 2016 - December 2016",
          "description": [
            "Setup infrastructure for Active directory using VMWare ESXI. Worked on Storage protocols such as SAN, NFS, ISCSI "
          ]
        }
      ],
      "skills": [
        {
          "group": "AI Infrastructure & LLM Platforms",
          "subSkills": [
            {"name": "LLM Gateway Development", "level": "90%"},
            {"name": "Vector Databases (Pinecone, OpenSearch)", "level": "85%"},
            {"name": "RAG Systems & Semantic Search", "level": "80%"},
            {"name": "Prompt Engineering & Optimization", "level": "85%"}
          ]
        },
        {
          "group": "Backend & Distributed Systems",
          "subSkills": [
            {"name": "Distributed Systems Architecture", "level": "95%"},
            {"name": "Microservices Design", "level": "90%"},
            {"name": "System Design & Scalability", "level": "95%"},
            {"name": "API Design (REST, GraphQL, gRPC)", "level": "90%"}
          ]
        },
        {
          "group": "MLOps & Model Deployment",
          "subSkills": [
            {"name": "AWS SageMaker", "level": "85%"},
            {"name": "Model Versioning & Registry", "level": "80%"},
            {"name": "A/B Testing & Canary Deployments", "level": "85%"},
            {"name": "Model Monitoring & Drift Detection", "level": "85%"}
          ]
        },
        {
          "group": "Cloud Infrastructure",
          "subSkills": [
            {"name": "AWS (EC2, S3, Lambda, ECS)", "level": "90%"},
            {"name": "Google Cloud Platform (GCP)", "level": "85%"},
            {"name": "Kubernetes & Container Orchestration", "level": "85%"},
            {"name": "Docker & Containerization", "level": "90%"},
            {"name": "Terraform & Infrastructure as Code", "level": "80%"}
          ]
        },
        {
          "group": "Programming Languages",
          "subSkills": [
            {"name": "Python", "level": "95%"},
            {"name": "Java & Spring Boot", "level": "80%"},
            {"name": "Go", "level": "70%"},
            {"name": "C++", "level": "60%"}
          ]
        },
        {
          "group": "Data & Messaging Systems",
          "subSkills": [
            {"name": "Kafka", "level": "90%"},
            {"name": "RabbitMQ & SQS", "level": "85%"},
            {"name": "PostgreSQL & MySQL", "level": "85%"},
            {"name": "Redis", "level": "90%"},
            {"name": "Elasticsearch", "level": "80%"}
          ]
        },
        {
          "group": "Observability & DevOps",
          "subSkills": [
            {"name": "Prometheus & Grafana", "level": "85%"},
            {"name": "Datadog & Splunk", "level": "85%"},
            {"name": "CI/CD Pipelines (Jenkins, GitHub Actions)", "level": "80%"},
            {"name": "Monitoring & Alerting", "level": "85%"}
          ]
        }
      ]
    },
    "portfolio": {
      "projects": [
        {
          "id": 1,
          "title": "Enterprise LLM Gateway Platform",
          "description": "Architected and built a production-grade LLM Gateway as a Platform Service enabling organization-wide AI adoption. Implemented unified API interfaces supporting multiple providers (OpenAI, Anthropic, Google, AWS Bedrock) with intelligent routing, rate limiting, cost tracking, and usage analytics. Built on Python FastAPI with Redis caching, PostgreSQL for metadata storage, and Kubernetes for orchestration. Features include request queuing, automatic failover, token management, and comprehensive observability with Prometheus and Datadog integration.",
          "category": "AI Infrastructure",
          "tags": "LLM, AI Platform, Python, FastAPI, Kubernetes, Redis, PostgreSQL, AWS",
          "image": "yelp.png",
          "url": "#",
          "modal": "modal-01"
        },
        {
          "id": 2,
          "title": "Vector Database Platform Service",
          "description": "Designed and implemented a scalable Vector Database Platform Service for semantic search and RAG applications. Built distributed embedding storage supporting millions of vectors with sub-millisecond query latency. Implemented horizontal scaling with sharding, replication, and load balancing. Features include similarity search, hybrid search (vector + keyword), automatic indexing, and multi-tenant isolation. Deployed on Kubernetes with OpenSearch/Pinecone integration, supporting real-time updates and batch ingestion pipelines.",
          "category": "AI Infrastructure",
          "tags": "Vector Database, Semantic Search, RAG, OpenSearch, Kubernetes, Distributed Systems",
          "image": "flight.jpg",
          "url": "#",
          "modal": "modal-02"
        },
        {
          "id": 3,
          "title": "ML Model Deployment Infrastructure on AWS SageMaker",
          "description": "Built production ML model deployment infrastructure on AWS SageMaker with 99.9% uptime SLA. Deployed sensitive models for document extraction and credit card approvals, processing $300M+ annually in transaction volume. Implemented automated model versioning, A/B testing framework, canary deployments, and automatic rollback mechanisms. Features include real-time and batch inference endpoints, model monitoring with drift detection, cost optimization through spot instances, and comprehensive logging/alerting.",
          "category": "MLOps",
          "tags": "MLOps, AWS SageMaker, Model Deployment, Python, PyTorch, XGBoost, Monitoring",
          "image": "openssl.jpg",
          "url": "#",
          "modal": "modal-03"
        },
        {
          "id": 4,
          "title": "Unified ML Model Deployment Pipeline",
          "description": "Led cross-functional initiative to unify ML model deployment infrastructure across organization, reducing deployment time by 90% and generating $5.2M annual revenue. Built CI/CD pipeline for ML models with automated testing, validation, and deployment. Migrated from legacy H2O platform to modern containerized deployment. Implemented model registry, feature store integration, and automated retraining pipelines. Technologies include Kubernetes, Docker, Terraform, Python, and various ML frameworks.",
          "category": "MLOps",
          "tags": "MLOps, CI/CD, Kubernetes, Docker, Terraform, Python, Model Registry",
          "image": "restapi.jpg",
          "url": "#",
          "modal": "modal-04"
        },
        {
          "id": 5,
          "title": "Document Intelligence Pipeline with AI",
          "description": "Architected Document Intelligence Pipeline with automated categorization, routing, and processing agents. Implemented multi-stage pipeline using Google Vision OCR, Document AI, and custom ML models for document classification and extraction. Built feedback loops for continuous model improvement with human-in-the-loop validation. Features include document queue management with Kafka, parallel processing, retry mechanisms, and comprehensive observability. Processes millions of documents monthly with high accuracy.",
          "category": "AI Infrastructure",
          "tags": "Document AI, OCR, ML Pipeline, Kafka, Python, Google Vision, Distributed Processing",
          "image": "ovx.png",
          "url": "#",
          "modal": "modal-05"
        },
        {
          "id": 6,
          "title": "High-Throughput Data Pipeline Optimization",
          "description": "Optimized data aggregation pipeline processing 10B+ events and 30M+ jobs daily, reducing job latency from 600s to 2.4s (99.6% improvement). Implemented distributed processing with Apache Spark, intelligent caching strategies with Redis, and optimized database queries. Built real-time streaming pipeline with Kafka and Flink for click/open analytics. Features include automatic scaling, fault tolerance, and comprehensive monitoring with Grafana dashboards.",
          "category": "Backend Infrastructure",
          "tags": "Data Pipeline, Apache Spark, Kafka, Redis, Performance Optimization, Distributed Systems",
          "image": "jenkins.png",
          "url": "#",
          "modal": "modal-06"
        },
        {
          "id": 7,
          "title": "Customer Data Platform with Change Data Capture",
          "description": "Architected Customer Data Platform backend infrastructure using Change Data Capture (CDC) with Kafka and Debezium. Created unified customer identity resolution system handling millions of records with real-time synchronization across multiple data sources. Implemented event-driven architecture with schema registry, data validation, and conflict resolution. Features include customer profile merging, deduplication, and real-time updates with sub-second latency.",
          "category": "Backend Infrastructure",
          "tags": "CDC, Kafka, Debezium, Data Engineering, Event-Driven Architecture, PostgreSQL",
          "image": "blog.jpg",
          "url": "#",
          "modal": "modal-07"
        },
        {
          "id": 8,
          "title": "Distributed ETL Pipeline with Auto-Healing",
          "description": "Built resilient distributed ETL pipeline infrastructure with automatic retry mechanisms, circuit breakers, and auto-healing capabilities. Implemented monitoring and alerting using Prometheus, Elasticsearch, and Grafana. Created custom Prometheus exporters for application metrics. Features include automatic failure recovery, pipeline health checks, and proactive alerting with PagerDuty integration. Reduced manual intervention by 80% and system downtime by 75%.",
          "category": "Backend Infrastructure",
          "tags": "ETL, Distributed Systems, Kubernetes, Prometheus, Observability, Python, Celery",
          "image": "imdb.jpg",
          "url": "#",
          "modal": "modal-08"
        }
      ]
    },
    "testimonials": {
      "testimonials": [
        {
          "text": "Deepanshu worked for me as a COOP at DELL EMC in 2016. He is a very quick and enthusiastic learner and very easy to work with. In a very short time, he was able to learn storage technology and was a valuable contributor to the team. He also learned VMware and setup the ESX server on his own. He reached out to team members when he needed help and worked well to adapt to changes. He worked on multiple projects and delivered on time. He will be an asset to any team and I will hire him in a minute if I had the chance!",
          "user": "Ms. Aparna Ramasundar(Manager at Dell EMC)"
        }
      ]
    }
  }
}